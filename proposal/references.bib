
@article{branisavljevicImprovedRealtimeData2011,
  title = {Improved Real-Time Data Anomaly Detection Using Context Classification},
  author = {Branisavljevi{\'c}, Nemanja and Kapelan, Zoran and Prodanovi{\'c}, Du{\v s}an},
  year = {2011},
  month = jul,
  journal = {Journal of Hydroinformatics},
  volume = {13},
  number = {3},
  pages = {307--323},
  issn = {1464-7141, 1465-1734},
  doi = {10.2166/hydro.2011.042},
  abstract = {The number of automated measuring and reporting systems used in water distribution and sewer systems is dramatically increasing and, as a consequence, so is the volume of data acquired. Since real-time data is likely to contain a certain amount of anomalous values and data acquisition equipment is not perfect, it is essential to equip the SCADA (Supervisory Control and Data Acquisition) system with automatic procedures that can detect the related problems and assist the user in monitoring and managing the incoming data. A number of different anomaly detection techniques and methods exist and can be used with varying success. To improve the performance, these methods must be fine tuned according to crucial aspects of the process monitored and the contexts in which the data are classified. The aim of this paper is to explore if the data context classification and pre-processing techniques can be used to improve the anomaly detection methods, especially in fully automated systems. The methodology developed is tested on sets of real-life data, using different standard and experimental anomaly detection procedures including statistical, model-based and data-mining approaches. The results obtained clearly demonstrate the effectiveness of the suggested anomaly detection methodology.},
  langid = {english},
  file = {/home/akail/Zotero/storage/CB278DEK/BranisavljeviÄ‡ et al. - 2011 - Improved real-time data anomaly detection using co.pdf}
}

@article{campbellQuantityNothingQuality2013,
  title = {Quantity Is {{Nothing}} without {{Quality}}: {{Automated QA}}/{{QC}} for {{Streaming Environmental Sensor Data}}},
  shorttitle = {Quantity Is {{Nothing}} without {{Quality}}},
  author = {Campbell, John L. and Rustad, Lindsey E. and Porter, John H. and Taylor, Jeffrey R. and Dereszynski, Ethan W. and Shanley, James B. and Gries, Corinna and Henshaw, Donald L. and Martin, Mary E. and Sheldon, Wade M. and Boose, Emery R.},
  year = {2013},
  month = jul,
  journal = {BioScience},
  volume = {63},
  number = {7},
  pages = {574--585},
  issn = {1525-3244, 0006-3568},
  doi = {10.1525/bio.2013.63.7.10},
  abstract = {Sensor networks are revolutionizing environmental monitoring by producing massive quantities of data that are being made publically available in near real time. These data streams pose a challenge for ecologists because traditional approaches to quality assurance and quality control are no longer practical when confronted with the size of these data sets and the demands of real-time processing. Automated methods for rapidly identifying and (ideally) correcting problematic data are essential. However, advances in sensor hardware have outpaced those in software, creating a need for tools to implement automated quality assurance and quality control procedures, produce graphical and statistical summaries for review, and track the provenance of the data. Use of automated tools would enhance data integrity and reliability and would reduce delays in releasing data products. Development of community-wide standards for quality assurance and quality control would instill confidence in sensor data and would improve interoperability across environmental sensor networks.},
  langid = {english},
  file = {/home/akail/Zotero/storage/JEMALNZ6/Campbell et al. - 2013 - Quantity is Nothing without Quality Automated QA.pdf}
}

@article{chandolaAnomalyDetectionSurveya,
  title = {Anomaly {{Detection}} : {{A Survey}}},
  author = {Chandola, Varun},
  pages = {72},
  langid = {english},
  file = {/home/akail/Zotero/storage/HP354JMX/Chandola - Anomaly Detection  A Survey.pdf}
}

@misc{EasybuildersEasybuildeasyconfigs,
  title = {Easybuilders/Easybuild-Easyconfigs},
  journal = {GitHub},
  abstract = {A collection of easyconfig files that describe which software to build using which build options with EasyBuild. - easybuilders/easybuild-easyconfigs},
  howpublished = {https://github.com/easybuilders/easybuild-easyconfigs},
  langid = {english},
  file = {/home/akail/Zotero/storage/QAMTHX6Y/issues.html}
}

@misc{ElectronicCodeFederal,
  type = {Text},
  title = {Electronic {{Code}} of {{Federal Regulations}} ({{eCFR}})},
  journal = {Electronic Code of Federal Regulations (eCFR)},
  abstract = {Electronic Code of Federal Regulations (eCFR) published by the Government Publishing Office},
  howpublished = {https://www.ecfr.gov/},
  langid = {english},
  file = {/home/akail/Zotero/storage/TMEJLP2J/retrieveECFR.html}
}

@article{elmanFindingStructureTime1990,
  title = {Finding Structure in Time},
  author = {Elman, Jeffrey L.},
  year = {1990},
  month = apr,
  journal = {Cognitive Science},
  volume = {14},
  number = {2},
  pages = {179--211},
  issn = {0364-0213},
  doi = {10.1016/0364-0213(90)90002-E},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
  langid = {english},
  file = {/home/akail/Zotero/storage/PC4K9NTD/Elman_1990_Finding structure in time.pdf;/home/akail/Zotero/storage/7XUIMT9I/036402139090002E.html}
}

@article{fiebrichQualityAssuranceProcedures2010,
  title = {Quality {{Assurance Procedures}} for {{Mesoscale Meteorological Data}}},
  author = {Fiebrich, Christopher A. and Morgan, Cynthia R. and McCombs, Alexandria G. and Hall, Peter K. and McPherson, Renee A.},
  year = {2010},
  month = oct,
  journal = {Journal of Atmospheric and Oceanic Technology},
  volume = {27},
  number = {10},
  pages = {1565--1582},
  issn = {1520-0426, 0739-0572},
  doi = {10.1175/2010JTECHA1433.1},
  abstract = {Mesoscale meteorological data present their own challenges and advantages during the quality assurance (QA) process because of their variability in both space and time. To ensure data quality, it is important to perform quality control at many different stages (e.g., sensor calibrations, automated tests, and manual assessment). As part of an ongoing refinement of quality assurance procedures, meteorologists with the Oklahoma Mesonet continually review advancements and techniques employed by other networks. This article's aim is to share those reviews and resources with scientists beginning or enhancing their own QA program. General QA considerations, general automated tests, and variable-specific tests and methods are discussed.},
  langid = {english},
  file = {/home/akail/Zotero/storage/XD6KZUJQ/Fiebrich et al. - 2010 - Quality Assurance Procedures for Mesoscale Meteoro.pdf}
}

@article{goernitzSupervisedAnomalyDetection2013,
  title = {Toward {{Supervised Anomaly Detection}}},
  author = {Goernitz, N. and Kloft, M. and Rieck, K. and Brefeld, U.},
  year = {2013},
  month = feb,
  journal = {Journal of Artificial Intelligence Research},
  volume = {46},
  pages = {235--262},
  issn = {1076-9757},
  doi = {10.1613/jair.3623},
  copyright = {Copyright (c)},
  langid = {english},
  file = {/home/akail/Zotero/storage/47W8EMSV/Goernitz et al_2013_Toward Supervised Anomaly Detection.pdf;/home/akail/Zotero/storage/B7JE25P9/10802.html;/home/akail/Zotero/storage/G3EZN7CV/10802.html}
}

@article{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  journal = {arXiv:1406.2661 [cs, stat]},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  file = {/home/akail/Zotero/storage/KHSZ7LMR/Goodfellow et al_2014_Generative Adversarial Networks.pdf;/home/akail/Zotero/storage/3RECHUE5/1406.html}
}

@article{graybealComplexQualityAssurance2004,
  title = {Complex {{Quality Assurance}} of {{Historical Hourly Surface Airways Meteorological Data}}},
  author = {Graybeal, Daniel Y and Degaetano, Arthur T and Eggleston, Keith L},
  year = {2004},
  journal = {JOURNAL OF ATMOSPHERIC AND OCEANIC TECHNOLOGY},
  volume = {21},
  pages = {14},
  abstract = {A new complex quality assurance (QA) procedure is developed for historical hourly surface airways meteorological data, recently digitized in a U.S. government\textendash sponsored effort that extends the digital period of record back as early as the late 1920s. The procedure builds upon an existing, three-pronged framework and incorporates several checks previously employed for hourly data. New or enhanced checks were also developed, partly with the goal of accommodating the peculiarities of historical observation practices or reporting standards, but also to improve our ability to flag suspicious data. The latter goal is sought by use of more elaborate physics or empirical relationships than are employed in existing checks, or by modifying or replacing them to sharpen their intended focus.},
  langid = {english},
  file = {/home/akail/Zotero/storage/RM49P8N6/Graybeal et al. - 2004 - Complex Quality Assurance of Historical Hourly Sur.pdf}
}

@article{hillakerDailyClimateData2009,
  title = {Daily Climate Data Quality Control Procedures of the {{Iowa State Climatologist}}},
  author = {Hillaker, Harry},
  year = {2009},
  month = jun,
  journal = {Journal of Applied and Service Climatology},
  volume = {2009},
  number = {1},
  issn = {26430223},
  doi = {10.46275/JoASC.2009.06.002},
  abstract = {The State Climatologist Office of the Iowa Department of Agriculture and Land Stewardship has been performing data entry and data quality control of National Oceanic and Atmospheric Administration daily climate data in Iowa since July 1, 1987. This process uses comprehensive, automated quality control tests based on standard instrumentation and observing practices and on standard climatological consistency. Inconsistencies flagged by these tests are manually resolved using a standard procedure based on information available from other sources and surrounding stations. The process then uses a manual spatial test to flag additional suspect values, which are also manually resolved using a standard procedure based on information available from other sources and surrounding stations. For example, for suspect values spotted in snowfall and snow depth spatial plots, visible satellite imagery may be consulted along with snowfall at neighboring stations to produce reasonable snowfall and snow depth estimates. This manually intensive process has produced a unique resource for comparison of manual quality control with automated processes, as well as analysis of Iowa climate.},
  langid = {english},
  file = {/home/akail/Zotero/storage/BCDEB5BW/Hillaker - 2009 - Daily climate data quality control procedures of t.pdf}
}

@misc{HomeATG,
  title = {Home - {{ATG}}},
  howpublished = {https://srnl.doe.gov/atg/index.html},
  file = {/home/akail/Zotero/storage/APTNJKXQ/index.html}
}

@article{liAnomalyDetectionTime2021,
  title = {Anomaly {{Detection}} of {{Time Series With Smoothness-Inducing Sequential Variational Auto-Encoder}}},
  author = {Li, L. and Yan, J. and Wang, H. and Jin, Y.},
  year = {2021},
  month = mar,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {3},
  pages = {1177--1191},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.2980749},
  abstract = {Deep generative models have demonstrated their effectiveness in learning latent representation and modeling complex dependencies of time series. In this article, we present a smoothness-inducing sequential variational auto-encoder (VAE) (SISVAE) model for the robust estimation and anomaly detection of multidimensional time series. Our model is based on VAE, and its backbone is fulfilled by a recurrent neural network to capture latent temporal structures of time series for both the generative model and the inference model. Specifically, our model parameterizes mean and variance for each time-stamp with flexible neural networks, resulting in a nonstationary model that can work without the assumption of constant noise as commonly made by existing Markov models. However, such flexibility may cause the model fragile to anomalies. To achieve robust density estimation which can also benefit detection tasks, we propose a smoothness-inducing prior over possible estimations. The proposed prior works as a regularizer that places penalty at nonsmooth reconstructions. Our model is learned efficiently with a novel stochastic gradient variational Bayes estimator. In particular, we study two decision criteria for anomaly detection: reconstruction probability and reconstruction error. We show the effectiveness of our model on both synthetic data sets and public real-world benchmarks.},
  keywords = {Adaptation models,Anomaly detection,Data models,deep generative model,Estimation,recurrent neural network,Recurrent neural networks,Robustness,time series,Time series analysis,variational auto-encoder (VAE)},
  file = {/home/akail/Zotero/storage/JGGAM5WN/Li et al_2021_Anomaly Detection of Time Series With Smoothness-Inducing Sequential.pdf;/home/akail/Zotero/storage/C4YJT3TI/9064715.html}
}

@article{malhotraLSTMbasedEncoderDecoderMultisensor2016,
  title = {{{LSTM-based Encoder-Decoder}} for {{Multi-sensor Anomaly Detection}}},
  author = {Malhotra, Pankaj and Ramakrishnan, Anusha and Anand, Gaurangi and Vig, Lovekesh and Agarwal, Puneet and Shroff, Gautam},
  year = {2016},
  month = jul,
  journal = {arXiv:1607.00148 [cs, stat]},
  eprint = {1607.00148},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).},
  archiveprefix = {arXiv},
  file = {/home/akail/Zotero/storage/IQD2IXJU/Malhotra et al. - 2016 - LSTM-based Encoder-Decoder for Multi-sensor Anomal.pdf;/home/akail/Zotero/storage/NVU4ZMST/1607.html}
}

@article{minhasSemisupervisedAnomalyDetection2019,
  title = {Semi-Supervised {{Anomaly Detection}} Using {{AutoEncoders}}},
  author = {Minhas, Manpreet Singh and Zelek, John},
  year = {2019},
  journal = {Journal of Computational Vision and Imaging Systems},
  volume = {5},
  number = {1},
  pages = {3--3},
  issn = {2562-0444},
  abstract = {Anomaly detection refers to the task of finding unusual instancesthat stand out from the normal data. In several applications, theseoutliers or anomalous instances are of greater interest compared tothe normal ones. Specifically in the case of industrial optical inspection and infrastructure asset management, finding these defects(anomalous regions) is of extreme importance. Traditionally andeven today this process has been carried out manually. Humansrely on the saliency of the defects in comparison to the normal texture to detect the defects. However, manual inspection is slow, tedious, subjective and susceptible to human biases. Therefore, theautomation of defect detection is desirable. But for defect detectionlack of availability of a large number of anomalous instances andlabelled data is a problem. In this paper, we present a convolutionalauto-encoder architecture for anomaly detection that is trained onlyon the defect-free (normal) instances. For the test images, residual masks that are obtained by subtracting the original image fromthe auto-encoder output are thresholded to obtain the defect segmentation masks. The approach was tested on two data-sets andachieved an impressive average F1 score of 0.885. The networklearnt to detect the actual shape of the defects even though no defected images were used during the training.},
  copyright = {Copyright (c)},
  langid = {english},
  file = {/home/akail/Zotero/storage/6UU7NFM4/Minhas and Zelek - 2019 - Semi-supervised Anomaly Detection using AutoEncode.pdf;/home/akail/Zotero/storage/IEGXW4LF/1654.html}
}

@misc{ML070630021Pdf,
  title = {{{ML070630021}}.Pdf},
  howpublished = {https://www.nrc.gov/docs/ML0706/ML070630021.pdf},
  file = {/home/akail/Zotero/storage/FJ2ZZGVP/ML070630021.pdf}
}

@article{northropQUALITYASSURANCEPROGRAM2019,
  title = {{{QUALITY ASSURANCE PROGRAM}}},
  author = {Northrop, C and Spencer, A},
  year = {2019},
  pages = {42},
  abstract = {OF REVISIONS: This instruction and National Weather Service Policy Directive 30-13 supersedes National Weather Service Policy Instruction, ``Quality Assurance Program,'' dated December 06, 2002. Changes were made to reflect the NWS Headquarters reorganization effective April 1, 2015.},
  langid = {english},
  file = {/home/akail/Zotero/storage/4YAHEIRE/Northrop and Spencer - 2019 - QUALITY ASSURANCE PROGRAM.pdf}
}

@inproceedings{onalWeatherDataAnalysis2017,
  title = {Weather Data Analysis and Sensor Fault Detection Using an Extended {{IoT}} Framework with Semantics, Big Data, and Machine Learning},
  booktitle = {2017 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Onal, A. C. and Sezer, O. Berat and Ozbayoglu, M. and Dogdu, E.},
  year = {2017},
  month = dec,
  pages = {2037--2046},
  doi = {10.1109/BigData.2017.8258150},
  abstract = {In recent years, big data and Internet of Things (IoT) implementations started getting more attention. Researchers focused on developing big data analytics solutions using machine learning models. Machine learning is a rising trend in this field due to its ability to extract hidden features and patterns even in highly complex datasets. In this study, we used our Big Data IoT Framework in a weather data analysis use case. We implemented weather clustering and sensor anomaly detection using a publicly available dataset. We provided the implementation details of each framework layer (acquisition, ETL, data processing, learning and decision) for this particular use case. Our chosen learning model within the library is Scikit-Learn based k-means clustering. The data analysis results indicate that it is possible to extract meaningful information from a relatively complex dataset using our framework.},
  file = {/home/akail/Zotero/storage/RKIP66KM/Onal et al_2017_Weather data analysis and sensor fault detection using an extended IoT.pdf;/home/akail/Zotero/storage/DCS5A84M/8258150.html}
}

@misc{PriceDropsLast,
  title = {Price {{Drops Last}} 24 {{Hours}}},
  howpublished = {https://pcpartpicker.com/products/pricedrop/},
  file = {/home/akail/Zotero/storage/AQHYEHWG/pricedrop.html}
}

@article{raissiHiddenPhysicsModels2018,
  title = {Hidden Physics Models: {{Machine}} Learning of Nonlinear Partial Differential Equations},
  shorttitle = {Hidden Physics Models},
  author = {Raissi, Maziar and Karniadakis, George Em},
  year = {2018},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {357},
  pages = {125--141},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2017.11.039},
  abstract = {While there is currently a lot of enthusiasm about ``big data'', useful data is usually ``small'' and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from small data. In particular, we introduce hidden physics models, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier\textendash Stokes, Schr\"odinger, Kuramoto\textendash Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.},
  langid = {english},
  keywords = {Bayesian modeling,Fractional equations,Probabilistic machine learning,Small data,System identification,Uncertainty quantification},
  annotation = {ZSCC: 0000239},
  file = {/home/akail/Zotero/storage/YVTECHXU/Raissi and Karniadakis - 2018 - Hidden physics models Machine learning of nonline.pdf;/home/akail/Zotero/storage/TQGFLKPM/S0021999117309014.html}
}

@article{ruffUnifyingReviewDeep2020,
  title = {A {{Unifying Review}} of {{Deep}} and {{Shallow Anomaly Detection}}},
  author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Gr{\'e}goire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and M{\"u}ller, Klaus-Robert},
  year = {2020},
  month = sep,
  journal = {arXiv:2009.11732 [cs, stat]},
  eprint = {2009.11732},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
  archiveprefix = {arXiv},
  file = {/home/akail/Zotero/storage/WPIQG7CV/Ruff et al. - 2020 - A Unifying Review of Deep and Shallow Anomaly Dete.pdf;/home/akail/Zotero/storage/T6LL44ZK/2009.html}
}

@misc{SaltcloudProxmoxDriver,
  title = {Salt-Cloud -- Proxmox Driver Doesn't Work. {$\cdot$} {{Issue}} \#43967 {$\cdot$} Saltstack/Salt},
  journal = {GitHub},
  abstract = {Description of Issue/Question Multiple issues in creating VMs and retrieving information from a working Proxmox cluster: salt-cloud --list-images doesn\&\#39;t list images salt-cloud -p \&lt;profile\&g...},
  howpublished = {https://github.com/saltstack/salt/issues/43967},
  langid = {english},
  file = {/home/akail/Zotero/storage/7ZXVWFL3/43967.html}
}

@article{sehwagSSDUNIFIEDFRAMEWORK2021,
  title = {{{SSD}}: {{A UNIFIED FRAMEWORK FOR SELF- SUPERVISED OUTLIER DETECTION}}},
  author = {Sehwag, Vikash and Chiang, Mung and Mittal, Prateek},
  year = {2021},
  pages = {17},
  abstract = {We ask the following question: what training information is required to design an effective outlier/out-of-distribution (OOD) detector, i.e., detecting samples that lie far away from the training distribution? Since unlabeled data is easily accessible for many applications, the most compelling approach is to develop detectors based on only unlabeled in-distribution data. However, we observe that most existing detectors based on unlabeled data perform poorly, often equivalent to a random prediction. In contrast, existing state-of-the-art OOD detectors achieve impressive performance but require access to fine-grained data labels for supervised training. We propose SSD, an outlier detector based on only unlabeled in-distribution data. We use self-supervised representation learning followed by a Mahalanobis distance based detection in the feature space. We demonstrate that SSD outperforms most existing detectors based on unlabeled data by a large margin. Additionally, SSD even achieves performance on par, and sometimes even better, with supervised training based detectors. Finally, we expand our detection framework with two key extensions. First, we formulate few-shot OOD detection, in which the detector has access to only one to five samples from each class of the targeted OOD dataset. Second, we extend our framework to incorporate training data labels, if available. We find that our novel detection framework based on SSD displays enhanced performance with these extensions, and achieves state-of-the-art performance1.},
  langid = {english},
  keywords = {Anomaly detection,self-supervised},
  file = {/home/akail/Zotero/storage/55KRQCTJ/Sehwag et al. - 2021 - SSD A UNIFIED FRAMEWORK FOR SELF- SUPERVISED OUTL.pdf}
}

@misc{SRNLTR202000197Pdf,
  title = {{{SRNL-TR-2020-00197}}.Pdf},
  howpublished = {https://srnl.doe.gov/atg/static/pdf/SRNL-TR-2020-00197.pdf},
  file = {/home/akail/Zotero/storage/MMAC27PD/SRNL-TR-2020-00197.pdf}
}

@article{venkatasubramanianReviewProcessFault2003,
  title = {A Review of Process Fault Detection and Diagnosis: {{Part I}}: {{Quantitative}} Model-Based Methods},
  shorttitle = {A Review of Process Fault Detection and Diagnosis},
  author = {Venkatasubramanian, Venkat and Rengaswamy, Raghunathan and Yin, Kewen and Kavuri, Surya N.},
  year = {2003},
  month = mar,
  journal = {Computers \& Chemical Engineering},
  volume = {27},
  number = {3},
  pages = {293--311},
  issn = {0098-1354},
  doi = {10.1016/S0098-1354(02)00160-6},
  abstract = {Fault detection and diagnosis is an important problem in process engineering. It is the central component of abnormal event management (AEM) which has attracted a lot of attention recently. AEM deals with the timely detection, diagnosis and correction of abnormal conditions of faults in a process. Early detection and diagnosis of process faults while the plant is still operating in a controllable region can help avoid abnormal event progression and reduce productivity loss. Since the petrochemical industries lose an estimated 20 billion dollars every year, they have rated AEM as their number one problem that needs to be solved. Hence, there is considerable interest in this field now from industrial practitioners as well as academic researchers, as opposed to a decade or so ago. There is an abundance of literature on process fault diagnosis ranging from analytical methods to artificial intelligence and statistical approaches. From a modelling perspective, there are methods that require accurate process models, semi-quantitative models, or qualitative models. At the other end of the spectrum, there are methods that do not assume any form of model information and rely only on historic process data. In addition, given the process knowledge, there are different search techniques that can be applied to perform diagnosis. Such a collection of bewildering array of methodologies and alternatives often poses a difficult challenge to any aspirant who is not a specialist in these techniques. Some of these ideas seem so far apart from one another that a non-expert researcher or practitioner is often left wondering about the suitability of a method for his or her diagnostic situation. While there have been some excellent reviews in this field in the past, they often focused on a particular branch, such as analytical models, of this broad discipline. The basic aim of this three part series of papers is to provide a systematic and comparative study of various diagnostic methods from different perspectives. We broadly classify fault diagnosis methods into three general categories and review them in three parts. They are quantitative model-based methods, qualitative model-based methods, and process history based methods. In the first part of the series, the problem of fault diagnosis is introduced and approaches based on quantitative models are reviewed. In the remaining two parts, methods based on qualitative models and process history data are reviewed. Furthermore, these disparate methods will be compared and evaluated based on a common set of criteria introduced in the first part of the series. We conclude the series with a discussion on the relationship of fault diagnosis to other process operations and on emerging trends such as hybrid blackboard-based frameworks for fault diagnosis.},
  langid = {english},
  keywords = {Diagnosis,Fault detection,Process safety},
  file = {/home/akail/Zotero/storage/9EQN9VTU/S0098135402001606.html}
}

@inproceedings{xiaoweixuDistributionbasedClusteringAlgorithm1998,
  title = {A Distribution-Based Clustering Algorithm for Mining in Large Spatial Databases},
  booktitle = {Proceedings 14th {{International Conference}} on {{Data Engineering}}},
  author = {{Xiaowei Xu} and Ester, M. and Kriegel, H.- and Sander, J.},
  year = {1998},
  month = feb,
  pages = {324--331},
  issn = {1063-6382},
  doi = {10.1109/ICDE.1998.655795},
  abstract = {The problem of detecting clusters of points belonging to a spatial point process arises in many applications. In this paper, we introduce the new clustering algorithm DBCLASD (Distribution-Based Clustering of LArge Spatial Databases) to discover clusters of this type. The results of experiments demonstrate that DBCLASD, contrary to partitioning algorithms such as CLARANS (Clustering Large Applications based on RANdomized Search), discovers clusters of arbitrary shape. Furthermore, DBCLASD does not require any input parameters, in contrast to the clustering algorithm DBSCAN (Density-Based Spatial Clustering of Applications with Noise) requiring two input parameters, which may be difficult to provide for large databases. In terms of efficiency, DBCLASD is between CLARANS and DBSCAN, close to DBSCAN. Thus, the efficiency of DBCLASD on large spatial databases is very attractive when considering its nonparametric nature and its good quality for clusters of arbitrary shape.},
  file = {/home/akail/Zotero/storage/HF974GES/Xiaowei Xu et al_1998_A distribution-based clustering algorithm for mining in large spatial databases.pdf;/home/akail/Zotero/storage/AUWKNG6U/655795.html}
}

@misc{zahumenskyGuidelinesQualityControl2004,
  title = {Guidelines on {{Quality Control Procedures}} for {{Data}} from {{Automatic Weather Stations}}},
  author = {Zahumensky, Igor},
  year = {2004},
  month = jul,
  publisher = {{WORLD METEOROLOGICAL ORGANIZATION}},
  abstract = {The document contains a proposal for the Guidelines on Quality Control Procedures for data from Automatic Weather Stations.},
  langid = {english},
  file = {/home/akail/Zotero/storage/LMGVZPER/WMO Data QC.pdf}
}


