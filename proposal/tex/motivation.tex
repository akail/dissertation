\section{Motivation}
\label{sec:motivation}

The forecast and simulation models developed and deployed by the \gls{atg} rely on quality input data  and therefore anomalous data may introduce unforeseen bias and error into their analysis. Instruments calibration drifts over time, equipment malfunctions or errors occur during the data processing pipeline. Currently the \gls{atg} employs a Quality Assurance/Quality Control (QA/QC) process to identify and classify “non-representative” records as anomalies.  This process trails the real-time needs for emergency response modeling and weather forecasting by several days or more and requires a lengthy manual and visual process to identify and record anomalies.  Patterns of malfunction may take weeks or months to become apparent, resulting in biased and misrepresented data feeding safety critical applications.

The industry standard for real-time \gls{qa} of meteorological data uses statistical analysis based on confidence intervals derived from climatological values. Prior experience at SRS and other facilities has revealed the limitations of such analyses with extreme weather events falsely classified as anomalous (Type I error). Increasing the confidence interval range mitigates this issue but may increase the number of misclassified anomalies (Type II errors). We seek to investigate the possibility of conducting real-time anomaly detection of meteorological data which takes into account the engineering controls, i.e. rules and range values, put in place by the procedures of the \gls{atg}, the unique geophysical characteristics of the \gls{srs}, the physical relationships and phenomena between data types to combine traditional anomaly detection methods in unsupervised learning with supervised deep learning to create an ensemble classification model.
